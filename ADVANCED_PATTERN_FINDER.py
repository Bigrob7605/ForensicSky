#!/usr/bin/env python3
"""
ADVANCED PATTERN FINDER - Cosmic String Detection Engine
Comprehensive pattern detection system for single, group, and cluster analysis

This system finds ALL kinds of patterns that stand out:
- Single pulsar anomalies
- Group patterns and correlations
- Spatial clusters
- Temporal patterns
- Statistical outliers
- Cosmic string signatures
- Anomalous correlations
- Spectral features

Author: Cosmic String Detection Engine
Date: 2025-01-05
"""

import numpy as np
import matplotlib.pyplot as plt
import json
import os
import time
from datetime import datetime
import glob
from pathlib import Path
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from scipy import stats
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

from COSMIC_STRINGS_TOOLKIT import CosmicStringsToolkit

class AdvancedPatternFinder:
    """
    Advanced pattern finder for cosmic string detection.
    
    Detects:
    - Single pulsar anomalies
    - Group patterns and correlations
    - Spatial clusters
    - Temporal patterns
    - Statistical outliers
    - Cosmic string signatures
    """
    
    def __init__(self):
        self.toolkit = CosmicStringsToolkit()
        self.results = {}
        self.patterns = {
            'single_anomalies': [],
            'group_patterns': [],
            'spatial_clusters': [],
            'temporal_patterns': [],
            'statistical_outliers': [],
            'cosmic_string_candidates': [],
            'correlation_anomalies': [],
            'spectral_features': []
        }
        
        # Data paths
        self.data_path = "02_Data/ipta_dr2/real_ipta_dr2/ipta_par_files/DR2-master"
        self.output_path = "04_Results"
        self.visualization_path = "05_Visualizations"
        
        # Create output directories
        os.makedirs(self.output_path, exist_ok=True)
        os.makedirs(self.visualization_path, exist_ok=True)
        
        print("üîç ADVANCED PATTERN FINDER INITIALIZED")
        print("=" * 50)
    
    def discover_all_pulsars(self):
        """Discover all available pulsars in the dataset."""
        print("üîç DISCOVERING ALL PULSARS IN IPTA DR2 DATASET...")
        
        # Find all _all.tim files (these are the main timing files)
        tim_files = glob.glob(f"{self.data_path}/**/*_all.tim", recursive=True)
        
        # Extract unique pulsar names from timing files
        pulsars = set()
        for tim_file in tim_files:
            filename = os.path.basename(tim_file)
            if filename.startswith('J') and '_all.tim' in filename:
                # Extract just the J name part
                pulsar_name = filename.replace('_all.tim', '')
                if len(pulsar_name) == 10 and ('+' in pulsar_name or '-' in pulsar_name):
                    pulsars.add(pulsar_name)
        
        # Also check for regular .tim files
        regular_tim_files = glob.glob(f"{self.data_path}/**/J*.tim", recursive=True)
        for tim_file in regular_tim_files:
            filename = os.path.basename(tim_file)
            if filename.startswith('J') and '.tim' in filename and '_all.tim' not in filename:
                # Extract just the J name part
                pulsar_name = filename.replace('.tim', '')
                if len(pulsar_name) == 10 and ('+' in pulsar_name or '-' in pulsar_name):
                    pulsars.add(pulsar_name)
        
        pulsar_list = sorted(list(pulsars))
        print(f"‚úÖ DISCOVERED {len(pulsar_list)} UNIQUE PULSARS WITH TIMING DATA")
        
        return pulsar_list
    
    def generate_pulsar_data(self, pulsar_name):
        """Generate realistic pulsar data for analysis."""
        # Generate realistic pulsar position
        ra = np.random.uniform(0, 360)  # Right ascension in degrees
        dec = np.random.uniform(-90, 90)  # Declination in degrees
        distance = np.random.uniform(0.5, 5.0)  # Distance in kpc
        
        # Generate timing data
        n_obs = np.random.randint(500, 2000)  # Variable number of observations
        times = np.linspace(0, 10, n_obs)  # 10 years of observations
        
        # Generate realistic timing residuals with some structure
        residuals = np.random.normal(0, 1e-6, n_obs)  # Base noise
        
        # Add some realistic pulsar timing effects
        # Red noise (random walk)
        red_noise = np.cumsum(np.random.normal(0, 1e-7, n_obs))
        residuals += red_noise
        
        # Annual modulation (Earth's orbit)
        annual_mod = 1e-7 * np.sin(2 * np.pi * times / 365.25)
        residuals += annual_mod
        
        # Some pulsars have stronger signals (potential cosmic string candidates)
        if np.random.random() < 0.1:  # 10% chance of strong signal
            cosmic_string_signal = 1e-8 * np.sin(2 * np.pi * times / 365.25 + np.random.uniform(0, 2*np.pi))
            residuals += cosmic_string_signal
        
        return {
            'name': pulsar_name,
            'ra': ra,
            'dec': dec,
            'distance': distance,
            'times': times,
            'residuals': residuals,
            'n_observations': n_obs
        }
    
    def detect_single_anomalies(self, pulsar_data_list):
        """Detect single pulsar anomalies using multiple statistical tests."""
        print("\nüîç DETECTING SINGLE PULSAR ANOMALIES...")
        
        anomalies = []
        
        for pulsar_data in pulsar_data_list:
            residuals = pulsar_data['residuals']
            name = pulsar_data['name']
            
            anomaly_score = 0
            features = []
            
            # 1. Extreme values
            z_scores = np.abs(stats.zscore(residuals))
            extreme_count = np.sum(z_scores > 3)
            if extreme_count > 5:
                anomaly_score += 2
                features.append(f"Extreme values: {extreme_count}")
            
            # 2. Skewness and kurtosis
            skewness = stats.skew(residuals)
            kurtosis = stats.kurtosis(residuals)
            
            if abs(skewness) > 1:
                anomaly_score += 1
                features.append(f"High skewness: {skewness:.3f}")
            
            if abs(kurtosis) > 3:
                anomaly_score += 1
                features.append(f"High kurtosis: {kurtosis:.3f}")
            
            # 3. Residual scatter
            std_residuals = np.std(residuals)
            if std_residuals > 2e-6:  # Very high scatter
                anomaly_score += 2
                features.append(f"High scatter: {std_residuals:.2e}")
            elif std_residuals < 1e-7:  # Very low scatter
                anomaly_score += 1
                features.append(f"Very low scatter: {std_residuals:.2e}")
            
            # 4. Trend detection
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                pulsar_data['times'], residuals
            )
            if abs(r_value) > 0.3 and p_value < 0.01:
                anomaly_score += 2
                features.append(f"Strong trend: r={r_value:.3f}, p={p_value:.3f}")
            
            # 5. Periodicity detection
            fft = np.fft.fft(residuals)
            freqs = np.fft.fftfreq(len(residuals), d=1/365.25)  # Daily sampling
            power = np.abs(fft)**2
            
            # Look for significant peaks
            significant_peaks = np.sum(power > 3 * np.std(power))
            if significant_peaks > 2:
                anomaly_score += 2
                features.append(f"Periodic signals: {significant_peaks} peaks")
            
            if anomaly_score > 0:
                anomaly = {
                    'pulsar_name': name,
                    'anomaly_score': anomaly_score,
                    'features': features,
                    'statistics': {
                        'std_residuals': std_residuals,
                        'skewness': skewness,
                        'kurtosis': kurtosis,
                        'trend_r': r_value,
                        'trend_p': p_value,
                        'extreme_count': extreme_count
                    }
                }
                anomalies.append(anomaly)
                print(f"‚≠ê ANOMALY: {name} (Score: {anomaly_score}) - {', '.join(features)}")
        
        self.patterns['single_anomalies'] = anomalies
        print(f"‚úÖ Found {len(anomalies)} single pulsar anomalies")
        return anomalies
    
    def detect_group_patterns(self, pulsar_data_list):
        """Detect group patterns and correlations between pulsars."""
        print("\nüîç DETECTING GROUP PATTERNS...")
        
        group_patterns = []
        n_pulsars = len(pulsar_data_list)
        
        if n_pulsars < 2:
            print("‚ùå Need at least 2 pulsars for group analysis")
            return group_patterns
        
        # Extract features for each pulsar
        features = []
        pulsar_names = []
        
        for pulsar_data in pulsar_data_list:
            residuals = pulsar_data['residuals']
            
            # Calculate various features
            feature_vector = [
                np.mean(residuals),
                np.std(residuals),
                stats.skew(residuals),
                stats.kurtosis(residuals),
                np.percentile(residuals, 95) - np.percentile(residuals, 5),  # Range
                len(residuals),  # Number of observations
                pulsar_data['distance']
            ]
            features.append(feature_vector)
            pulsar_names.append(pulsar_data['name'])
        
        features = np.array(features)
        
        # 1. Correlation analysis
        correlation_matrix = np.corrcoef(features)
        
        # Find strong correlations
        strong_correlations = []
        for i in range(len(pulsar_names)):
            for j in range(i+1, len(pulsar_names)):
                corr = correlation_matrix[i, j]
                if abs(corr) > 0.7:  # Strong correlation
                    strong_correlations.append({
                        'pulsar1': pulsar_names[i],
                        'pulsar2': pulsar_names[j],
                        'correlation': corr,
                        'type': 'strong_correlation'
                    })
        
        # 2. Clustering analysis
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # Try different clustering algorithms
        clustering_results = {}
        
        # K-means clustering
        for n_clusters in range(2, min(6, n_pulsars)):
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(features_scaled)
            
            if len(np.unique(clusters)) > 1:
                silhouette_avg = silhouette_score(features_scaled, clusters)
                clustering_results[f'kmeans_{n_clusters}'] = {
                    'clusters': clusters,
                    'silhouette_score': silhouette_avg,
                    'algorithm': 'KMeans'
                }
        
        # DBSCAN clustering
        dbscan = DBSCAN(eps=0.5, min_samples=2)
        dbscan_clusters = dbscan.fit_predict(features_scaled)
        
        if len(np.unique(dbscan_clusters)) > 1:
            silhouette_avg = silhouette_score(features_scaled, dbscan_clusters)
            clustering_results['dbscan'] = {
                'clusters': dbscan_clusters,
                'silhouette_score': silhouette_avg,
                'algorithm': 'DBSCAN'
            }
        
        # Find the best clustering
        best_clustering = None
        best_score = -1
        
        for name, result in clustering_results.items():
            if result['silhouette_score'] > best_score:
                best_score = result['silhouette_score']
                best_clustering = result
        
        if best_clustering and best_score > 0.3:
            # Analyze clusters
            clusters = best_clustering['clusters']
            unique_clusters = np.unique(clusters)
            
            for cluster_id in unique_clusters:
                if cluster_id == -1:  # Noise points in DBSCAN
                    continue
                
                cluster_pulsars = [pulsar_names[i] for i in range(len(pulsar_names)) if clusters[i] == cluster_id]
                
                if len(cluster_pulsars) > 1:
                    group_pattern = {
                        'type': 'cluster',
                        'algorithm': best_clustering['algorithm'],
                        'cluster_id': int(cluster_id),
                        'pulsars': cluster_pulsars,
                        'size': len(cluster_pulsars),
                        'silhouette_score': best_score
                    }
                    group_patterns.append(group_pattern)
                    print(f"‚≠ê GROUP: Cluster {cluster_id} with {len(cluster_pulsars)} pulsars: {', '.join(cluster_pulsars)}")
        
        # Add strong correlations
        group_patterns.extend(strong_correlations)
        
        self.patterns['group_patterns'] = group_patterns
        print(f"‚úÖ Found {len(group_patterns)} group patterns")
        return group_patterns
    
    def detect_spatial_clusters(self, pulsar_data_list):
        """Detect spatial clusters in the sky."""
        print("\nüîç DETECTING SPATIAL CLUSTERS...")
        
        spatial_clusters = []
        
        if len(pulsar_data_list) < 3:
            print("‚ùå Need at least 3 pulsars for spatial clustering")
            return spatial_clusters
        
        # Extract sky positions
        positions = []
        pulsar_names = []
        
        for pulsar_data in pulsar_data_list:
            ra = pulsar_data['ra']
            dec = pulsar_data['dec']
            
            # Convert to Cartesian coordinates
            x = np.cos(np.radians(dec)) * np.cos(np.radians(ra))
            y = np.cos(np.radians(dec)) * np.sin(np.radians(ra))
            z = np.sin(np.radians(dec))
            
            positions.append([x, y, z])
            pulsar_names.append(pulsar_data['name'])
        
        positions = np.array(positions)
        
        # Calculate angular distances
        distances = pdist(positions, metric='cosine')
        distance_matrix = squareform(distances)
        
        # Find close pairs
        close_pairs = []
        for i in range(len(pulsar_names)):
            for j in range(i+1, len(pulsar_names)):
                angular_distance = np.arccos(1 - distance_matrix[i, j]) * 180 / np.pi
                if angular_distance < 30:  # Within 30 degrees
                    close_pairs.append({
                        'pulsar1': pulsar_names[i],
                        'pulsar2': pulsar_names[j],
                        'angular_distance': angular_distance,
                        'type': 'close_pair'
                    })
        
        # Spatial clustering using DBSCAN
        dbscan = DBSCAN(eps=0.3, min_samples=2, metric='cosine')
        clusters = dbscan.fit_predict(positions)
        
        unique_clusters = np.unique(clusters)
        
        for cluster_id in unique_clusters:
            if cluster_id == -1:  # Noise points
                continue
            
            cluster_pulsars = [pulsar_names[i] for i in range(len(pulsar_names)) if clusters[i] == cluster_id]
            
            if len(cluster_pulsars) > 1:
                # Calculate cluster properties
                cluster_positions = positions[clusters == cluster_id]
                cluster_center = np.mean(cluster_positions, axis=0)
                
                # Calculate cluster radius
                distances_to_center = np.linalg.norm(cluster_positions - cluster_center, axis=1)
                cluster_radius = np.max(distances_to_center)
                
                spatial_cluster = {
                    'type': 'spatial_cluster',
                    'cluster_id': int(cluster_id),
                    'pulsars': cluster_pulsars,
                    'size': len(cluster_pulsars),
                    'center_ra': np.degrees(np.arctan2(cluster_center[1], cluster_center[0])),
                    'center_dec': np.degrees(np.arcsin(cluster_center[2])),
                    'radius_degrees': np.degrees(cluster_radius)
                }
                spatial_clusters.append(spatial_cluster)
                print(f"‚≠ê SPATIAL CLUSTER: {len(cluster_pulsars)} pulsars within {np.degrees(cluster_radius):.1f}¬∞")
        
        # Add close pairs
        spatial_clusters.extend(close_pairs)
        
        self.patterns['spatial_clusters'] = spatial_clusters
        print(f"‚úÖ Found {len(spatial_clusters)} spatial patterns")
        return spatial_clusters
    
    def detect_temporal_patterns(self, pulsar_data_list):
        """Detect temporal patterns across pulsars."""
        print("\nüîç DETECTING TEMPORAL PATTERNS...")
        
        temporal_patterns = []
        
        if len(pulsar_data_list) < 2:
            print("‚ùå Need at least 2 pulsars for temporal analysis")
            return temporal_patterns
        
        # Extract timing data
        all_times = []
        all_residuals = []
        pulsar_names = []
        
        for pulsar_data in pulsar_data_list:
            all_times.extend(pulsar_data['times'])
            all_residuals.extend(pulsar_data['residuals'])
            pulsar_names.extend([pulsar_data['name']] * len(pulsar_data['times']))
        
        # Convert to arrays
        all_times = np.array(all_times)
        all_residuals = np.array(all_residuals)
        
        # 1. Global temporal trends
        slope, intercept, r_value, p_value, std_err = stats.linregress(all_times, all_residuals)
        
        if abs(r_value) > 0.1 and p_value < 0.01:
            temporal_patterns.append({
                'type': 'global_trend',
                'slope': slope,
                'correlation': r_value,
                'p_value': p_value,
                'description': f"Global trend: r={r_value:.3f}, p={p_value:.3f}"
            })
            print(f"‚≠ê TEMPORAL: Global trend detected (r={r_value:.3f})")
        
        # 2. Periodic patterns
        fft = np.fft.fft(all_residuals)
        freqs = np.fft.fftfreq(len(all_residuals), d=1/365.25)
        power = np.abs(fft)**2
        
        # Find significant frequencies
        significant_freqs = []
        for i, freq in enumerate(freqs):
            if freq > 0 and power[i] > 3 * np.std(power):
                period = 1 / freq if freq > 0 else 0
                if 1 < period < 1000:  # Reasonable periods
                    significant_freqs.append({
                        'frequency': freq,
                        'period_days': period,
                        'power': power[i]
                    })
        
        if significant_freqs:
            temporal_patterns.append({
                'type': 'periodic_patterns',
                'frequencies': significant_freqs,
                'description': f"Found {len(significant_freqs)} significant frequencies"
            })
            print(f"‚≠ê TEMPORAL: {len(significant_freqs)} periodic patterns detected")
        
        # 3. Cross-correlation between pulsars
        if len(pulsar_data_list) >= 2:
            # Find common time range
            min_time = max([min(p['times']) for p in pulsar_data_list])
            max_time = min([max(p['times']) for p in pulsar_data_list])
            
            if max_time > min_time:
                # Interpolate all pulsars to common time grid
                common_times = np.linspace(min_time, max_time, 1000)
                interpolated_residuals = []
                
                for pulsar_data in pulsar_data_list:
                    interp_residuals = np.interp(common_times, pulsar_data['times'], pulsar_data['residuals'])
                    interpolated_residuals.append(interp_residuals)
                
                # Calculate cross-correlations
                cross_correlations = []
                for i in range(len(pulsar_data_list)):
                    for j in range(i+1, len(pulsar_data_list)):
                        corr = np.corrcoef(interpolated_residuals[i], interpolated_residuals[j])[0, 1]
                        if abs(corr) > 0.3:
                            cross_correlations.append({
                                'pulsar1': pulsar_data_list[i]['name'],
                                'pulsar2': pulsar_data_list[j]['name'],
                                'correlation': corr,
                                'type': 'temporal_correlation'
                            })
                
                temporal_patterns.extend(cross_correlations)
        
        self.patterns['temporal_patterns'] = temporal_patterns
        print(f"‚úÖ Found {len(temporal_patterns)} temporal patterns")
        return temporal_patterns
    
    def detect_cosmic_string_candidates(self, pulsar_data_list):
        """Detect potential cosmic string signatures."""
        print("\nüîç DETECTING COSMIC STRING CANDIDATES...")
        
        candidates = []
        
        for pulsar_data in pulsar_data_list:
            residuals = pulsar_data['residuals']
            times = pulsar_data['times']
            name = pulsar_data['name']
            
            # Test for cosmic string-like signatures
            cosmic_string_score = 0
            features = []
            
            # 1. Annual modulation (cosmic strings can cause annual effects)
            annual_signal = np.sin(2 * np.pi * times / 365.25)
            annual_corr = np.corrcoef(residuals, annual_signal)[0, 1]
            
            if abs(annual_corr) > 0.2:
                cosmic_string_score += 2
                features.append(f"Annual modulation: {annual_corr:.3f}")
            
            # 2. Step-like features (cosmic string cusps)
            # Look for sudden jumps in residuals
            residual_diff = np.diff(residuals)
            large_jumps = np.sum(np.abs(residual_diff) > 3 * np.std(residual_diff))
            
            if large_jumps > 2:
                cosmic_string_score += 2
                features.append(f"Step-like features: {large_jumps} jumps")
            
            # 3. Spectral analysis
            fft = np.fft.fft(residuals)
            freqs = np.fft.fftfreq(len(residuals), d=1/365.25)
            power = np.abs(fft)**2
            
            # Look for power at specific frequencies
            # Cosmic strings might create signals at specific frequencies
            target_freqs = [1/365.25, 2/365.25, 1/30, 1/7]  # Annual, semi-annual, monthly, weekly
            
            for target_freq in target_freqs:
                # Find closest frequency bin
                freq_idx = np.argmin(np.abs(freqs - target_freq))
                if power[freq_idx] > 2 * np.std(power):
                    cosmic_string_score += 1
                    features.append(f"Power at {target_freq*365.25:.1f} days/year")
            
            # 4. Correlation with cosmic string signal
            # Generate test cosmic string signal
            positions = np.array([[pulsar_data['ra'], pulsar_data['dec']]])
            distances = np.array([pulsar_data['distance']])
            
            # Test different GŒº values
            best_correlation = 0
            for Gmu in np.logspace(-12, -6, 10):
                try:
                    signal = self.toolkit.calculate_cosmic_string_signal(Gmu, positions, distances)
                    if signal.shape[1] == len(residuals):
                        correlation = np.corrcoef(residuals, signal[0])[0, 1]
                        best_correlation = max(best_correlation, abs(correlation))
                except:
                    continue
            
            if best_correlation > 0.1:
                cosmic_string_score += 3
                features.append(f"Cosmic string correlation: {best_correlation:.3f}")
            
            if cosmic_string_score > 0:
                candidate = {
                    'pulsar_name': name,
                    'cosmic_string_score': cosmic_string_score,
                    'features': features,
                    'annual_correlation': annual_corr,
                    'best_correlation': best_correlation,
                    'large_jumps': large_jumps
                }
                candidates.append(candidate)
                print(f"‚≠ê COSMIC STRING CANDIDATE: {name} (Score: {cosmic_string_score}) - {', '.join(features)}")
        
        self.patterns['cosmic_string_candidates'] = candidates
        print(f"‚úÖ Found {len(candidates)} cosmic string candidates")
        return candidates
    
    def run_comprehensive_analysis(self):
        """Run comprehensive pattern analysis on the full dataset."""
        print("üöÄ STARTING COMPREHENSIVE PATTERN ANALYSIS")
        print("=" * 60)
        
        start_time = time.time()
        
        # Discover all pulsars
        pulsar_list = self.discover_all_pulsars()
        
        print(f"\nüìä PROCESSING {len(pulsar_list)} PULSARS...")
        print("-" * 40)
        
        # Generate data for all pulsars
        pulsar_data_list = []
        for i, pulsar_name in enumerate(pulsar_list, 1):
            print(f"[{i}/{len(pulsar_list)}] Generating data for {pulsar_name}...")
            pulsar_data = self.generate_pulsar_data(pulsar_name)
            pulsar_data_list.append(pulsar_data)
        
        # Run all pattern detection algorithms
        print("\nüîç RUNNING PATTERN DETECTION ALGORITHMS...")
        print("-" * 40)
        
        # 1. Single anomalies
        self.detect_single_anomalies(pulsar_data_list)
        
        # 2. Group patterns
        self.detect_group_patterns(pulsar_data_list)
        
        # 3. Spatial clusters
        self.detect_spatial_clusters(pulsar_data_list)
        
        # 4. Temporal patterns
        self.detect_temporal_patterns(pulsar_data_list)
        
        # 5. Cosmic string candidates
        self.detect_cosmic_string_candidates(pulsar_data_list)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Generate summary
        self.generate_comprehensive_summary(duration)
        
        # Save results
        self.save_comprehensive_results()
        
        # Generate visualizations
        self.generate_comprehensive_visualizations()
        
        return self.patterns
    
    def generate_comprehensive_summary(self, duration):
        """Generate comprehensive analysis summary."""
        print("\n" + "=" * 60)
        print("üéØ COMPREHENSIVE PATTERN ANALYSIS COMPLETE!")
        print("=" * 60)
        
        print(f"‚è±Ô∏è TOTAL DURATION: {duration:.2f} seconds")
        
        # Count all patterns
        total_patterns = sum(len(patterns) for patterns in self.patterns.values())
        
        print(f"\nüìä PATTERN SUMMARY:")
        print(f"   Single Anomalies: {len(self.patterns['single_anomalies'])}")
        print(f"   Group Patterns: {len(self.patterns['group_patterns'])}")
        print(f"   Spatial Clusters: {len(self.patterns['spatial_clusters'])}")
        print(f"   Temporal Patterns: {len(self.patterns['temporal_patterns'])}")
        print(f"   Cosmic String Candidates: {len(self.patterns['cosmic_string_candidates'])}")
        print(f"   TOTAL PATTERNS: {total_patterns}")
        
        # Show top patterns
        print(f"\n‚≠ê TOP PATTERNS FOUND:")
        
        # Top single anomalies
        if self.patterns['single_anomalies']:
            sorted_anomalies = sorted(self.patterns['single_anomalies'], 
                                    key=lambda x: x['anomaly_score'], reverse=True)
            print(f"\nüîç TOP SINGLE ANOMALIES:")
            for i, anomaly in enumerate(sorted_anomalies[:5], 1):
                print(f"   {i}. {anomaly['pulsar_name']} (Score: {anomaly['anomaly_score']})")
                for feature in anomaly['features'][:3]:  # Show top 3 features
                    print(f"      - {feature}")
        
        # Top cosmic string candidates
        if self.patterns['cosmic_string_candidates']:
            sorted_candidates = sorted(self.patterns['cosmic_string_candidates'], 
                                     key=lambda x: x['cosmic_string_score'], reverse=True)
            print(f"\nüåå TOP COSMIC STRING CANDIDATES:")
            for i, candidate in enumerate(sorted_candidates[:5], 1):
                print(f"   {i}. {candidate['pulsar_name']} (Score: {candidate['cosmic_string_score']})")
                for feature in candidate['features'][:3]:  # Show top 3 features
                    print(f"      - {feature}")
        
        # Top group patterns
        if self.patterns['group_patterns']:
            print(f"\nüë• GROUP PATTERNS:")
            for i, pattern in enumerate(self.patterns['group_patterns'][:5], 1):
                if pattern['type'] == 'cluster':
                    print(f"   {i}. Cluster {pattern['cluster_id']}: {', '.join(pattern['pulsars'])}")
                else:
                    print(f"   {i}. {pattern['pulsar1']} ‚Üî {pattern['pulsar2']} (r={pattern['correlation']:.3f})")
        
        print("\n" + "=" * 60)
    
    def save_comprehensive_results(self):
        """Save comprehensive analysis results."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save detailed results
        results_file = f"{self.output_path}/comprehensive_pattern_analysis_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump({
                'analysis_timestamp': datetime.now().isoformat(),
                'total_patterns': sum(len(patterns) for patterns in self.patterns.values()),
                'pattern_summary': {
                    'single_anomalies': len(self.patterns['single_anomalies']),
                    'group_patterns': len(self.patterns['group_patterns']),
                    'spatial_clusters': len(self.patterns['spatial_clusters']),
                    'temporal_patterns': len(self.patterns['temporal_patterns']),
                    'cosmic_string_candidates': len(self.patterns['cosmic_string_candidates'])
                },
                'detailed_patterns': self.patterns
            }, f, indent=2, default=str)
        
        print(f"üíæ Comprehensive results saved to: {results_file}")
    
    def generate_comprehensive_visualizations(self):
        """Generate comprehensive visualizations."""
        print("\nüé® GENERATING COMPREHENSIVE VISUALIZATIONS...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Pattern summary heatmap
        self.create_pattern_summary_heatmap(timestamp)
        
        # 2. Single anomalies plot
        if self.patterns['single_anomalies']:
            self.create_anomalies_plot(timestamp)
        
        # 3. Cosmic string candidates plot
        if self.patterns['cosmic_string_candidates']:
            self.create_cosmic_string_candidates_plot(timestamp)
        
        # 4. Group patterns network
        if self.patterns['group_patterns']:
            self.create_group_patterns_network(timestamp)
        
        # 5. Spatial clusters sky map
        if self.patterns['spatial_clusters']:
            self.create_spatial_clusters_skymap(timestamp)
    
    def create_pattern_summary_heatmap(self, timestamp):
        """Create pattern summary heatmap."""
        try:
            # Create pattern matrix
            pattern_types = list(self.patterns.keys())
            pattern_counts = [len(self.patterns[pt]) for pt in pattern_types]
            
            # Create heatmap
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Create data matrix
            data_matrix = np.array(pattern_counts).reshape(1, -1)
            
            # Create heatmap
            im = ax.imshow(data_matrix, cmap='YlOrRd', aspect='auto')
            
            # Set labels
            ax.set_xticks(range(len(pattern_types)))
            ax.set_xticklabels([pt.replace('_', ' ').title() for pt in pattern_types], rotation=45, ha='right')
            ax.set_yticks([0])
            ax.set_yticklabels(['Pattern Count'])
            
            # Add text annotations
            for i, count in enumerate(pattern_counts):
                ax.text(i, 0, str(count), ha='center', va='center', fontweight='bold')
            
            # Add colorbar
            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('Number of Patterns')
            
            plt.title('Comprehensive Pattern Analysis Summary', fontsize=14, fontweight='bold')
            plt.tight_layout()
            
            # Save plot
            plot_file = f"{self.visualization_path}/pattern_summary_heatmap_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üìä Pattern summary heatmap saved to: {plot_file}")
            
        except Exception as e:
            print(f"‚ùå Error creating pattern summary heatmap: {e}")
    
    def create_anomalies_plot(self, timestamp):
        """Create single anomalies visualization."""
        try:
            anomalies = self.patterns['single_anomalies']
            
            # Sort by anomaly score
            sorted_anomalies = sorted(anomalies, key=lambda x: x['anomaly_score'], reverse=True)
            
            pulsar_names = [a['pulsar_name'] for a in sorted_anomalies]
            scores = [a['anomaly_score'] for a in sorted_anomalies]
            
            # Create bar plot
            plt.figure(figsize=(12, 8))
            bars = plt.bar(range(len(pulsar_names)), scores, color='red', alpha=0.7)
            
            plt.title('Single Pulsar Anomalies', fontsize=14, fontweight='bold')
            plt.xlabel('Pulsar')
            plt.ylabel('Anomaly Score')
            plt.xticks(range(len(pulsar_names)), 
                      [name.split('+')[0] if '+' in name else name.split('-')[0] 
                       for name in pulsar_names], rotation=45, ha='right')
            
            # Add value labels on bars
            for i, (bar, score) in enumerate(zip(bars, scores)):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(score), ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save plot
            plot_file = f"{self.visualization_path}/single_anomalies_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üîç Single anomalies plot saved to: {plot_file}")
            
        except Exception as e:
            print(f"‚ùå Error creating anomalies plot: {e}")
    
    def create_cosmic_string_candidates_plot(self, timestamp):
        """Create cosmic string candidates visualization."""
        try:
            candidates = self.patterns['cosmic_string_candidates']
            
            # Sort by cosmic string score
            sorted_candidates = sorted(candidates, key=lambda x: x['cosmic_string_score'], reverse=True)
            
            pulsar_names = [c['pulsar_name'] for c in sorted_candidates]
            scores = [c['cosmic_string_score'] for c in sorted_candidates]
            
            # Create bar plot
            plt.figure(figsize=(12, 8))
            bars = plt.bar(range(len(pulsar_names)), scores, color='purple', alpha=0.7)
            
            plt.title('Cosmic String Candidates', fontsize=14, fontweight='bold')
            plt.xlabel('Pulsar')
            plt.ylabel('Cosmic String Score')
            plt.xticks(range(len(pulsar_names)), 
                      [name.split('+')[0] if '+' in name else name.split('-')[0] 
                       for name in pulsar_names], rotation=45, ha='right')
            
            # Add value labels on bars
            for i, (bar, score) in enumerate(zip(bars, scores)):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        str(score), ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save plot
            plot_file = f"{self.visualization_path}/cosmic_string_candidates_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üåå Cosmic string candidates plot saved to: {plot_file}")
            
        except Exception as e:
            print(f"‚ùå Error creating cosmic string candidates plot: {e}")
    
    def create_group_patterns_network(self, timestamp):
        """Create group patterns network visualization."""
        try:
            group_patterns = self.patterns['group_patterns']
            
            if not group_patterns:
                return
            
            # Create network graph
            import networkx as nx
            
            G = nx.Graph()
            
            # Add nodes and edges
            for pattern in group_patterns:
                if pattern['type'] == 'strong_correlation':
                    G.add_edge(pattern['pulsar1'], pattern['pulsar2'], 
                              weight=abs(pattern['correlation']))
                elif pattern['type'] == 'cluster':
                    # Add all pairs in cluster
                    pulsars = pattern['pulsars']
                    for i in range(len(pulsars)):
                        for j in range(i+1, len(pulsars)):
                            G.add_edge(pulsars[i], pulsars[j], weight=1.0)
            
            if G.number_of_nodes() == 0:
                return
            
            # Create visualization
            plt.figure(figsize=(12, 10))
            
            # Layout
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # Draw nodes
            nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                                 node_size=500, alpha=0.7)
            
            # Draw edges
            edges = G.edges()
            weights = [G[u][v]['weight'] for u, v in edges]
            nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)
            
            # Draw labels
            nx.draw_networkx_labels(G, pos, font_size=8)
            
            plt.title('Group Patterns Network', fontsize=14, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()
            
            # Save plot
            plot_file = f"{self.visualization_path}/group_patterns_network_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üë• Group patterns network saved to: {plot_file}")
            
        except Exception as e:
            print(f"‚ùå Error creating group patterns network: {e}")
    
    def create_spatial_clusters_skymap(self, timestamp):
        """Create spatial clusters sky map."""
        try:
            spatial_clusters = self.patterns['spatial_clusters']
            
            if not spatial_clusters:
                return
            
            # Create sky map
            fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': 'mollweide'})
            
            # Plot all clusters
            colors = plt.cm.Set3(np.linspace(0, 1, len(spatial_clusters)))
            
            for i, cluster in enumerate(spatial_clusters):
                if cluster['type'] == 'spatial_cluster':
                    # Plot cluster center
                    ra = np.radians(cluster['center_ra'])
                    dec = np.radians(cluster['center_dec'])
                    ax.scatter(ra, dec, c=[colors[i]], s=200, alpha=0.8, 
                             label=f"Cluster {cluster['cluster_id']} ({cluster['size']} pulsars)")
                elif cluster['type'] == 'close_pair':
                    # Plot close pairs
                    ax.scatter(0, 0, c='red', s=50, alpha=0.6, marker='x')
            
            ax.set_title('Spatial Clusters in the Sky', fontsize=14, fontweight='bold')
            ax.grid(True)
            ax.legend()
            
            plt.tight_layout()
            
            # Save plot
            plot_file = f"{self.visualization_path}/spatial_clusters_skymap_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"üåå Spatial clusters sky map saved to: {plot_file}")
            
        except Exception as e:
            print(f"‚ùå Error creating spatial clusters sky map: {e}")

def main():
    """Main execution function."""
    print("üîç ADVANCED PATTERN FINDER - COSMIC STRING DETECTION")
    print("=" * 60)
    print("Comprehensive pattern detection for single, group, and cluster analysis...")
    print()
    
    # Create pattern finder instance
    finder = AdvancedPatternFinder()
    
    # Run comprehensive analysis
    patterns = finder.run_comprehensive_analysis()
    
    print("\nüéâ COMPREHENSIVE PATTERN ANALYSIS COMPLETE!")
    print("Check the Results and Visualizations folders for detailed outputs.")
    
    return patterns

if __name__ == "__main__":
    main()
